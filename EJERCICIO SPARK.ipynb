{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, row_number, countDistinct, lit, sum, avg, row_number, md5, sha2, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType, FloatType, DecimalType\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "hive_context = HiveContext(sc)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EJERCICIO SPARK\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parquet_products_path = \"/home/training/data/products/part-00000-tid-2452134987220853249-b7b7ad2c-5a72-4006-a202-614e63e70c99-5-1-c000.snappy.parquet\"\n",
    "parquet_sellers_path = \"/home/training/data/sellers/part-00000-tid-6580020465882417462-624fcc11-67f0-4cff-abb7-804016c31f95-218-1-c000.snappy.parquet\"\n",
    "parquet_sales_path = \"/home/training/data/sales/part-00000-tid-8977413855564310585-22625781-9ad7-4f0e-b3ed-fb62a0788d8f-227-1-c000.snappy.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_products = spark.read.parquet(\"file://\" + parquet_products_path)\n",
    "df_sellers = spark.read.parquet(\"file://\" + parquet_sellers_path)\n",
    "df_sales = spark.read.parquet(\"file://\" + parquet_sales_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         0|   product_0|   22|\n",
      "|         1|   product_1|   30|\n",
      "|         2|   product_2|   91|\n",
      "|         3|   product_3|   37|\n",
      "|         4|   product_4|  145|\n",
      "|         5|   product_5|  128|\n",
      "|         6|   product_6|   66|\n",
      "|         7|   product_7|  145|\n",
      "|         8|   product_8|   51|\n",
      "|         9|   product_9|   44|\n",
      "|        10|  product_10|   53|\n",
      "|        11|  product_11|   13|\n",
      "|        12|  product_12|  104|\n",
      "|        13|  product_13|  102|\n",
      "|        14|  product_14|   24|\n",
      "|        15|  product_15|   14|\n",
      "|        16|  product_16|   38|\n",
      "|        17|  product_17|   72|\n",
      "|        18|  product_18|   16|\n",
      "|        19|  product_19|   46|\n",
      "+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|      806542|\n",
      "|        2|   seller_2|      623640|\n",
      "|        3|   seller_3|     1687106|\n",
      "|        4|   seller_4|       27991|\n",
      "|        5|   seller_5|     1652820|\n",
      "|        6|   seller_6|      179508|\n",
      "|        7|   seller_7|      401090|\n",
      "|        8|   seller_8|      670016|\n",
      "|        9|   seller_9|     1203705|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sellers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-06|             64|pqctnxzofwmtwlpsz...|\n",
      "|       2|         0|        0|2020-07-03|             84|olapxfcjktuajsuwp...|\n",
      "|       3|         0|        0|2020-07-09|             86|ksrwojmeqpfipdaed...|\n",
      "|       4|         0|        0|2020-07-09|             71|gpfifiykcpiwfftvo...|\n",
      "|       5|         0|        0|2020-07-02|             90|qafhecifvdljsflyj...|\n",
      "|       6|         0|        0|2020-07-08|             43|zxhorgqqououyqnmn...|\n",
      "|       7|         0|        0|2020-07-08|              7|uypazquoqxxevgqff...|\n",
      "|       8|         0|        0|2020-07-09|             68|wwchozldqkostuang...|\n",
      "|       9|         0|        0|2020-07-08|             40|xglmmsgbiuclmwkpn...|\n",
      "|      10|         0|        0|2020-07-09|             78|ejtgoapbwptkhgnhs...|\n",
      "|      11|         0|        0|2020-07-03|             44|gyzbmrvmgkthjobod...|\n",
      "|      12|         0|        0|2020-07-05|              7|gladpurwsbqvesdgz...|\n",
      "|      13|         0|        0|2020-07-04|             52|szuihdpyqpqaubdwg...|\n",
      "|      14|         0|        0|2020-07-08|             76|ztaiiiepyksrfpgco...|\n",
      "|      15|         0|        0|2020-07-08|             38|dhoezgeyefkmwwbch...|\n",
      "|      16|         0|        0|2020-07-07|             74|shdqilxigwbokuhnf...|\n",
      "|      17|         0|        0|2020-07-03|             50|dioemfaozjhrlxLln...|\n",
      "|      18|         0|        0|2020-07-01|             21|bmsmtucsgrdujkslt...|\n",
      "|      19|         0|        0|2020-07-02|             14|arlvnwswjqfBoqerh...|\n",
      "|      20|         0|        0|2020-07-02|             41|xcmvetvlrwsyccpgr...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- daily_target: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_pieces_sold: string (nullable = true)\n",
      " |-- bill_raw_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.printSchema()\n",
    "df_sellers.printSchema()\n",
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_products = df_products.select(col('product_id').cast(IntegerType()),col('product_name'),col('price').cast(DecimalType()))\n",
    "df_sellers = df_sellers.select(col('seller_id').cast(IntegerType()),col('seller_name'),col('daily_target').cast(IntegerType()))\n",
    "df_sales = df_sales.select(col('order_id').cast(IntegerType()),col('product_id').cast(IntegerType()),col('seller_id').cast(IntegerType()),col('date').cast(DateType()),col('num_pieces_sold').cast(IntegerType()),col('bill_raw_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: decimal(10,0) (nullable = true)\n",
      "\n",
      "root\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- daily_target: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- num_pieces_sold: integer (nullable = true)\n",
      " |-- bill_raw_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.printSchema()\n",
    "df_sellers.printSchema()\n",
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Averigüe cuántos pedidos, cuántos productos y cuántos vendedores hay en los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sellers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400040"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|cantidad_de_productos|\n",
      "+---------------------+\n",
      "|                50000|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.createOrReplaceTempView(\"df_products\")\n",
    "nproductos = spark.sql(\"SELECT count(*) as cantidad_de_productos from df_products\")\n",
    "nproductos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|cantidad_de_vendedores|\n",
      "+----------------------+\n",
      "|                    10|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sellers.createOrReplaceTempView(\"df_sellers\")\n",
    "nproductos = spark.sql(\"SELECT count(*) as cantidad_de_vendedores from df_sellers\")\n",
    "nproductos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|cantidad_de_pedidos|\n",
      "+-------------------+\n",
      "|             400040|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.createOrReplaceTempView(\"df_sales\")\n",
    "nproductos = spark.sql(\"SELECT count(*) as cantidad_de_pedidos from df_sales\")\n",
    "nproductos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b ¿Cuántos productos se han vendido al menos una vez?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT product_id)|\n",
      "+--------------------------+\n",
      "|                     16515|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_distinct = df_sales.select(countDistinct(\"product_id\"))\n",
    "sales_distinct.show()\n",
    "\n",
    "#df_sales.select(\"product_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|productos_unicos_vendidos|\n",
      "+-------------------------+\n",
      "|                    16515|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_distinct = spark.sql(\"select count(distinct(product_id)) as productos_unicos_vendidos from df_sales \")\n",
    "products_distinct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c ¿Cuál es el producto contenido en más pedidos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|product_id| count|\n",
      "+----------+------+\n",
      "|         0|380000|\n",
      "+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy('product_id').count().orderBy(col('count').desc()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|product_id|cantidad_vendida|\n",
      "+----------+----------------+\n",
      "|         0|          380000|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_mas_vendido = spark.sql(\"select product_id, count(*) as cantidad_vendida \\\n",
    "                                from df_sales \\\n",
    "                                group by product_id \\\n",
    "                                order by cantidad_vendida desc\\\n",
    "                                limit 1\")\n",
    "prod_mas_vendido.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ¿Cuántos productos distintos se han vendido en cada día?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------+\n",
      "|      date|cantidad de productos distintos|\n",
      "+----------+-------------------------------+\n",
      "|2020-07-01|                           1997|\n",
      "|2020-07-02|                           1942|\n",
      "|2020-07-03|                           1908|\n",
      "|2020-07-04|                           2001|\n",
      "|2020-07-05|                           1976|\n",
      "|2020-07-06|                           1977|\n",
      "|2020-07-07|                           1903|\n",
      "|2020-07-08|                           2004|\n",
      "|2020-07-09|                           2005|\n",
      "|2020-07-10|                           1968|\n",
      "+----------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dist_day = df_sales.groupBy(\"date\")\\\n",
    "                        .agg(countDistinct(\"product_id\").alias(\"cantidad de productos distintos\"))\\\n",
    "                        .orderBy(\"date\")\n",
    "df_dist_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+\n",
      "|      date|cantidad_productos_unicos|\n",
      "+----------+-------------------------+\n",
      "|2020-07-01|                     1997|\n",
      "|2020-07-02|                     1942|\n",
      "|2020-07-03|                     1908|\n",
      "|2020-07-04|                     2001|\n",
      "|2020-07-05|                     1976|\n",
      "|2020-07-06|                     1977|\n",
      "|2020-07-07|                     1903|\n",
      "|2020-07-08|                     2004|\n",
      "|2020-07-09|                     2005|\n",
      "|2020-07-10|                     1968|\n",
      "+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_distinct_day = spark.sql(\"select date, count(distinct(product_id)) as cantidad_productos_unicos\\\n",
    "                                from df_sales\\\n",
    "                                group by date\\\n",
    "                                order by date\")\n",
    "prod_distinct_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ¿Cuál es el ingreso promedio de los pedidos? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+-----+--------+\n",
      "|order_id|product_id|num_pieces_sold|price|subtotal|\n",
      "+--------+----------+---------------+-----+--------+\n",
      "|       1|         0|             64|   22|    1408|\n",
      "|       2|         0|             84|   22|    1848|\n",
      "|       3|         0|             86|   22|    1892|\n",
      "|       4|         0|             71|   22|    1562|\n",
      "|       5|         0|             90|   22|    1980|\n",
      "|       6|         0|             43|   22|     946|\n",
      "|       7|         0|              7|   22|     154|\n",
      "|       8|         0|             68|   22|    1496|\n",
      "|       9|         0|             40|   22|     880|\n",
      "|      10|         0|             78|   22|    1716|\n",
      "|      11|         0|             44|   22|     968|\n",
      "|      12|         0|              7|   22|     154|\n",
      "|      13|         0|             52|   22|    1144|\n",
      "|      14|         0|             76|   22|    1672|\n",
      "|      15|         0|             38|   22|     836|\n",
      "|      16|         0|             74|   22|    1628|\n",
      "|      17|         0|             50|   22|    1100|\n",
      "|      18|         0|             21|   22|     462|\n",
      "|      19|         0|             14|   22|     308|\n",
      "|      20|         0|             41|   22|     902|\n",
      "+--------+----------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join dataframes df_sales and df_products\n",
    "df_sales_prod = df_sales.join(df_products,[\"product_id\"])\n",
    "## add subtotal column\n",
    "df_sales_prod = df_sales_prod.select(\n",
    "    \"order_id\",\n",
    "    \"product_id\",\n",
    "    \"num_pieces_sold\",\n",
    "    \"price\",\n",
    "    F.lit(df_sales_prod.num_pieces_sold * df_sales_prod.price).alias(\"subtotal\")\n",
    ")\n",
    "df_sales_prod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|avg(subtotal)|\n",
      "+-------------+\n",
      "|    1247.0087|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find average \n",
    "#df_sales_prod.agg({'subtotal': 'avg'}).show()\n",
    "df_sales_prod.select(avg('subtotal')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|promedio_ventas|\n",
      "+---------------+\n",
      "|      1247.0087|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_subtotal = spark.sql(\"select avg(p.price*s.num_pieces_sold) as promedio_ventas\\\n",
    "                            from df_sales s join df_products p \\\n",
    "                            on s.product_id = p.product_id\")\n",
    "avg_subtotal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Para cada vendedor, ¿cuál es el porcentaje promedio de contribución de un pedido a la cuota diaria del vendedor? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n",
      "|seller_id|avg_contribution_perc|\n",
      "+---------+---------------------+\n",
      "|        0|           0.76832768|\n",
      "|        1| 0.014035598890076401|\n",
      "|        2| 0.017860464370470144|\n",
      "|        3| 0.006408251763671044|\n",
      "|        4|   0.4040727376656782|\n",
      "|        5| 0.006950908144867561|\n",
      "|        6| 0.059841901196604055|\n",
      "|        7| 0.029155551123189304|\n",
      "|        8| 0.017612564476072214|\n",
      "|        9| 0.009113860954303589|\n",
      "+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_sales.groupby(\"seller_id\",\"date\").agg(sum(\"num_pieces_sold\").alias(\"sold_by_date\")).orderBy(\"seller_id\")\n",
    "df_4 = df.join(df_sellers.alias(\"sellers\"), [\"seller_id\"])\\\n",
    "            .withColumn(\"ratio\",col(\"sold_by_date\")/col(\"daily_target\"))\\\n",
    "            .groupby(\"seller_id\").agg(avg(\"ratio\").alias(\"avg_contribution_perc\"))\n",
    "df_4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n",
      "|seller_id|avg_contribution_perc|\n",
      "+---------+---------------------+\n",
      "|        0|           0.76832768|\n",
      "|        1| 0.014035598890076401|\n",
      "|        2| 0.017860464370470144|\n",
      "|        3| 0.006408251763671044|\n",
      "|        4|   0.4040727376656782|\n",
      "|        5| 0.006950908144867561|\n",
      "|        6| 0.059841901196604055|\n",
      "|        7| 0.029155551123189304|\n",
      "|        8| 0.017612564476072214|\n",
      "|        9| 0.009113860954303589|\n",
      "+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_contribucion = spark.sql(\"select s.seller_id, avg(s.sum_pieces/v.daily_target) as avg_contribution_perc\\\n",
    "                                from df_sellers v join \\\n",
    "                                    (select seller_id, date, sum(num_pieces_sold) as sum_pieces \\\n",
    "                                    from df_sales \\\n",
    "                                    group by seller_id, date) s\\\n",
    "                                on v.seller_id = s.seller_id\\\n",
    "                                group by s.seller_id\\\n",
    "                                order by s.seller_id\"\n",
    "                            )\n",
    "avg_contribucion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a ¿Quiénes son las segundas personas que más venden de cada producto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+---+\n",
      "|product_id|seller_id|n_piezas_vendidas|row|\n",
      "+----------+---------+-----------------+---+\n",
      "|        13|        8|                6|  2|\n",
      "|        18|        8|               22|  2|\n",
      "|        30|        5|               56|  2|\n",
      "|        57|        9|               74|  2|\n",
      "|        65|        7|               22|  2|\n",
      "|        74|        6|               81|  2|\n",
      "|        88|        8|               50|  2|\n",
      "|        99|        7|               71|  2|\n",
      "|       103|        4|               10|  2|\n",
      "|       129|        9|               47|  2|\n",
      "|       136|        8|               41|  2|\n",
      "|       153|        5|               13|  2|\n",
      "|       175|        1|               28|  2|\n",
      "|       194|        1|                4|  2|\n",
      "|       195|        6|               25|  2|\n",
      "|       203|        9|               26|  2|\n",
      "|       228|        3|                6|  2|\n",
      "|       260|        3|               45|  2|\n",
      "|       262|        9|               40|  2|\n",
      "|       303|        7|                6|  2|\n",
      "+----------+---------+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = df_sales.groupBy('product_id','seller_id').agg(sum('num_pieces_sold').alias(\"n_piezas_vendidas\")).orderBy(\"product_id\")\n",
    "\n",
    "windowPart2 = Window.partitionBy(\"product_id\").orderBy(col(\"n_piezas_vendidas\").desc())\n",
    "df_5 = temp.withColumn(\"row\",row_number().over(windowPart2)) \\\n",
    "            .where(col(\"row\")==2).select(\"product_id\",\"seller_id\",\"n_piezas_vendidas\",\"row\")\n",
    "\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+---+\n",
      "|product_id|seller_id|pieces_sold|row|\n",
      "+----------+---------+-----------+---+\n",
      "|        13|        8|          6|  2|\n",
      "|        18|        8|         22|  2|\n",
      "|        30|        5|         56|  2|\n",
      "|        57|        9|         74|  2|\n",
      "|        65|        7|         22|  2|\n",
      "|        74|        6|         81|  2|\n",
      "|        88|        8|         50|  2|\n",
      "|        99|        7|         71|  2|\n",
      "|       103|        4|         10|  2|\n",
      "|       129|        9|         47|  2|\n",
      "|       136|        8|         41|  2|\n",
      "|       153|        5|         13|  2|\n",
      "|       175|        1|         28|  2|\n",
      "|       194|        1|          4|  2|\n",
      "|       195|        6|         25|  2|\n",
      "|       203|        9|         26|  2|\n",
      "|       228|        3|          6|  2|\n",
      "|       260|        3|         45|  2|\n",
      "|       262|        9|         40|  2|\n",
      "|       303|        7|          6|  2|\n",
      "+----------+---------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_sellers2 = spark.sql(\"select * from\\\n",
    "                                (select product_id, seller_id, pieces_sold,\\\n",
    "                                row_number() over(partition by product_id order by pieces_sold desc) as row\\\n",
    "                                from (select product_id, seller_id, sum(num_pieces_sold) as pieces_sold\\\n",
    "                                            from df_sales\\\n",
    "                                            group by product_id, seller_id \\\n",
    "                                            order by product_id))\\\n",
    "                        where row = 2\")\n",
    "max_sellers2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿y las que menos venden (vendedores) de cada producto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+---+\n",
      "|product_id|seller_id|n_piezas_vendidas|row|\n",
      "+----------+---------+-----------------+---+\n",
      "|         0|        0|         19208192|  1|\n",
      "|         1|        7|               25|  1|\n",
      "|         2|        1|               28|  1|\n",
      "|         5|        4|               35|  1|\n",
      "|         6|        8|               68|  1|\n",
      "|        12|        8|              100|  1|\n",
      "|        13|        8|                6|  1|\n",
      "|        18|        8|               22|  1|\n",
      "|        19|        7|               51|  1|\n",
      "|        26|        6|               47|  1|\n",
      "|        30|        5|               56|  1|\n",
      "|        34|        2|               42|  1|\n",
      "|        36|        2|               25|  1|\n",
      "|        37|        2|               32|  1|\n",
      "|        39|        1|               68|  1|\n",
      "|        42|        2|               39|  1|\n",
      "|        44|        9|               70|  1|\n",
      "|        47|        8|               55|  1|\n",
      "|        48|        1|               33|  1|\n",
      "|        49|        5|               38|  1|\n",
      "+----------+---------+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowPart3 = Window.partitionBy(\"product_id\").orderBy(\"n_piezas_vendidas\")\n",
    "df_5b = temp.withColumn(\"row\",row_number().over(windowPart3)) \\\n",
    "            .where(col(\"row\")==1).select(\"product_id\",\"seller_id\",\"n_piezas_vendidas\",\"row\")\n",
    "\n",
    "df_5b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+---+\n",
      "|product_id|seller_id|pieces_sold|row|\n",
      "+----------+---------+-----------+---+\n",
      "|         0|        0|   19208192|  1|\n",
      "|         1|        7|         25|  1|\n",
      "|         2|        1|         28|  1|\n",
      "|         5|        4|         35|  1|\n",
      "|         6|        8|         68|  1|\n",
      "|        12|        8|        100|  1|\n",
      "|        13|        8|          6|  1|\n",
      "|        18|        8|         22|  1|\n",
      "|        19|        7|         51|  1|\n",
      "|        26|        6|         47|  1|\n",
      "|        30|        5|         56|  1|\n",
      "|        34|        2|         42|  1|\n",
      "|        36|        2|         25|  1|\n",
      "|        37|        2|         32|  1|\n",
      "|        39|        1|         68|  1|\n",
      "|        42|        2|         39|  1|\n",
      "|        44|        9|         70|  1|\n",
      "|        47|        8|         55|  1|\n",
      "|        48|        1|         33|  1|\n",
      "|        49|        5|         38|  1|\n",
      "+----------+---------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from\\\n",
    "            (select product_id, seller_id, sum(num_pieces_sold) as pieces_sold,\\\n",
    "                    row_number() over(partition by product_id order by product_id) as row\\\n",
    "                    from df_sales \\\n",
    "                    group by product_id, seller_id \\\n",
    "                    order by product_id)\\\n",
    "            where row = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ¿Quiénes son los del producto con `product_id = 0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n",
      "|product_id|seller_id|n_piezas_vendidas|\n",
      "+----------+---------+-----------------+\n",
      "|         0|        0|         19208192|\n",
      "+----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp.where(temp.product_id == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|product_id|seller_id|pieces_sold|\n",
      "+----------+---------+-----------+\n",
      "|         0|        0|   19208192|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "spark.sql(\"select product_id, seller_id, sum(num_pieces_sold) as pieces_sold\\\n",
    "                from df_sales\\\n",
    "                where product_id = 0\\\n",
    "                group by product_id, seller_id\\\n",
    "                order by product_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cree una nueva columna llamada \"hashed_bill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_6 = df_sales.select(\"order_id\",\"bill_raw_text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|order_id|       bill_raw_text|         hashed_bill|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|pqctnxzofwmtwlpsz...|8bc043c5b5667b430...|\n",
      "|       2|olapxfcjktuajsuwp...|73d3cb489a6d7f404...|\n",
      "|       3|ksrwojmeqpfipdaed...|138702f6603eb97e4...|\n",
      "|       4|gpfifiykcpiwfftvo...|e31df5d21e8278451...|\n",
      "|       5|qafhecifvdljsflyj...|62f23b475f2a654d8...|\n",
      "|       6|zxhorgqqououyqnmn...|8953e20d1c635ba25...|\n",
      "|       7|uypazquoqxxevgqff...|4f8f4cc13ea3527f1...|\n",
      "|       8|wwchozldqkostuang...|5187c7545e9f82b6b...|\n",
      "|       9|xglmmsgbiuclmwkpn...|195ed7180971fe395...|\n",
      "|      10|ejtgoapbwptkhgnhs...|c377e2166b4a5b518...|\n",
      "|      11|gyzbmrvmgkthjobod...|1e826ede7ec9fa247...|\n",
      "|      12|gladpurwsbqvesdgz...|25fc1145163435832...|\n",
      "|      13|szuihdpyqpqaubdwg...|2ea8ab55840b84f8e...|\n",
      "|      14|ztaiiiepyksrfpgco...|5335a077d42a688d9...|\n",
      "|      15|dhoezgeyefkmwwbch...|5ad62c9f4c32732b1...|\n",
      "|      16|shdqilxigwbokuhnf...|b69adf59ef59b6d8d...|\n",
      "|      17|dioemfaozjhrlxLln...|a459ec06c6c026501...|\n",
      "|      18|bmsmtucsgrdujkslt...|994f1912713033746...|\n",
      "|      19|arlvnwswjqfBoqerh...|28a6349ad4c2d3d5d...|\n",
      "|      20|xcmvetvlrwsyccpgr...|54c2e2da5f0e0d852...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probando el hash md5 en la columna bill_raw_text\n",
    "df_6.withColumn(\"hashed_bill\", md5(df_6.bill_raw_text)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|order_id|       bill_raw_text|         hashed_bill|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|pqctnxzofwmtwlpsz...|a01585b2954e28192...|\n",
      "|       2|olapxfcjktuajsuwp...|c159e77eda8927078...|\n",
      "|       3|ksrwojmeqpfipdaed...|7d30cb71e384aa843...|\n",
      "|       4|gpfifiykcpiwfftvo...|acd83bea9abb6a323...|\n",
      "|       5|qafhecifvdljsflyj...|342de8e8affa5a01e...|\n",
      "|       6|zxhorgqqououyqnmn...|6d7ac6bf1fa2b43f7...|\n",
      "|       7|uypazquoqxxevgqff...|f2555e65c65446e71...|\n",
      "|       8|wwchozldqkostuang...|0f01f25d46a0ad27e...|\n",
      "|       9|xglmmsgbiuclmwkpn...|ee1b6eec80c3197b9...|\n",
      "|      10|ejtgoapbwptkhgnhs...|493bf07cd19b52d38...|\n",
      "|      11|gyzbmrvmgkthjobod...|c6592baef11c6b997...|\n",
      "|      12|gladpurwsbqvesdgz...|3b67d8525f9ad5da2...|\n",
      "|      13|szuihdpyqpqaubdwg...|eb00949f0279c0825...|\n",
      "|      14|ztaiiiepyksrfpgco...|00eac44507ef793f7...|\n",
      "|      15|dhoezgeyefkmwwbch...|389e5b181fa80691b...|\n",
      "|      16|shdqilxigwbokuhnf...|3793c9cff9308cee0...|\n",
      "|      17|dioemfaozjhrlxLln...|c0909cd254ae8a895...|\n",
      "|      18|bmsmtucsgrdujkslt...|4c5979d1b3eacb9cf...|\n",
      "|      19|arlvnwswjqfBoqerh...|7007ee518cdc6f1fc...|\n",
      "|      20|xcmvetvlrwsyccpgr...|77b4fc0713b0ecacb...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probando el hash sha256 en la columna bill_raw_text\n",
    "df_6.withColumn(\"hashed_bill\", sha2(df_6.bill_raw_text,256)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creacion de una UDF Function para iterar el md5\n",
    "def hashmd5(text): \n",
    "    return text\n",
    "    n = text.count(\"A\")\n",
    "    for i in range(n):\n",
    "        text = md5(text)\n",
    "    return text\n",
    "\n",
    "md5UDF = spark.udf.register(\"hashmd5\", hashmd5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- bill_raw_text: string (nullable = true)\n",
      " |-- hashed_bill: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o359.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 100.0 failed 1 times, most recent failure: Lost task 0.0 in stage 100.0 (TID 5291, localhost, executor driver): org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/usr/lib64/python2.6/runpy.py\", line 104, in _run_module_as_main\n      loader, code, fname = _get_module_details(mod_name)\n    File \"/usr/lib64/python2.6/runpy.py\", line 79, in _get_module_details\n      loader = get_loader(mod_name)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 456, in get_loader\n      return find_loader(fullname)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 466, in find_loader\n      for importer in iter_importers(fullname):\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 422, in iter_importers\n      __import__(pkg)\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 51, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/context.py\", line 31, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 97, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 72, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 246, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 270, in CloudPickler\n  NameError: name 'memoryview' is not defined\nPYTHONPATH was:\n  /opt/hadoop/spark/python/lib/pyspark.zip:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/jars/spark-core_2.11-2.4.8.jar:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/::/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/\norg.apache.spark.SparkException: No port number in pyspark.daemon's stdout\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:204)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:128)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/usr/lib64/python2.6/runpy.py\", line 104, in _run_module_as_main\n      loader, code, fname = _get_module_details(mod_name)\n    File \"/usr/lib64/python2.6/runpy.py\", line 79, in _get_module_details\n      loader = get_loader(mod_name)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 456, in get_loader\n      return find_loader(fullname)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 466, in find_loader\n      for importer in iter_importers(fullname):\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 422, in iter_importers\n      __import__(pkg)\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 51, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/context.py\", line 31, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 97, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 72, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 246, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 270, in CloudPickler\n  NameError: name 'memoryview' is not defined\nPYTHONPATH was:\n  /opt/hadoop/spark/python/lib/pyspark.zip:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/jars/spark-core_2.11-2.4.8.jar:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/::/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/\norg.apache.spark.SparkException: No port number in pyspark.daemon's stdout\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:204)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:128)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1c41bb47f558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hashed_bill\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_id\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5UDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbill_raw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msha2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbill_raw_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_hash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_hash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/hadoop/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hadoop/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o359.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 100.0 failed 1 times, most recent failure: Lost task 0.0 in stage 100.0 (TID 5291, localhost, executor driver): org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/usr/lib64/python2.6/runpy.py\", line 104, in _run_module_as_main\n      loader, code, fname = _get_module_details(mod_name)\n    File \"/usr/lib64/python2.6/runpy.py\", line 79, in _get_module_details\n      loader = get_loader(mod_name)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 456, in get_loader\n      return find_loader(fullname)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 466, in find_loader\n      for importer in iter_importers(fullname):\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 422, in iter_importers\n      __import__(pkg)\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 51, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/context.py\", line 31, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 97, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 72, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 246, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 270, in CloudPickler\n  NameError: name 'memoryview' is not defined\nPYTHONPATH was:\n  /opt/hadoop/spark/python/lib/pyspark.zip:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/jars/spark-core_2.11-2.4.8.jar:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/::/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/\norg.apache.spark.SparkException: No port number in pyspark.daemon's stdout\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:204)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:128)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Traceback (most recent call last):\n    File \"/usr/lib64/python2.6/runpy.py\", line 104, in _run_module_as_main\n      loader, code, fname = _get_module_details(mod_name)\n    File \"/usr/lib64/python2.6/runpy.py\", line 79, in _get_module_details\n      loader = get_loader(mod_name)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 456, in get_loader\n      return find_loader(fullname)\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 466, in find_loader\n      for importer in iter_importers(fullname):\n    File \"/usr/lib64/python2.6/pkgutil.py\", line 422, in iter_importers\n      __import__(pkg)\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/__init__.py\", line 51, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/context.py\", line 31, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 97, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 72, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 246, in <module>\n    File \"/opt/hadoop/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 270, in CloudPickler\n  NameError: name 'memoryview' is not defined\nPYTHONPATH was:\n  /opt/hadoop/spark/python/lib/pyspark.zip:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/jars/spark-core_2.11-2.4.8.jar:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/::/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/:/opt/hadoop/spark/python/lib/py4j-0.10.7-src.zip:/opt/hadoop/spark/python/\norg.apache.spark.SparkException: No port number in pyspark.daemon's stdout\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:204)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:128)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#Uso de la UDF en el filtro del order_id\n",
    "df_hash = df_6.withColumn(\"hashed_bill\", when(df_6.order_id % 2 == 0, md5UDF(df_6.bill_raw_text))\\\n",
    "                                      .otherwise(sha2(df_6.bill_raw_text,256)))\n",
    "df_hash.printSchema()\n",
    "df_hash.show()\n",
    "#aqui me genera un error que desconozco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|order_id|       bill_raw_text|         hashed_bill|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|pqctnxzofwmtwlpsz...|a01585b2954e28192...|\n",
      "|       2|olapxfcjktuajsuwp...|73d3cb489a6d7f404...|\n",
      "|       3|ksrwojmeqpfipdaed...|7d30cb71e384aa843...|\n",
      "|       4|gpfifiykcpiwfftvo...|e31df5d21e8278451...|\n",
      "|       5|qafhecifvdljsflyj...|342de8e8affa5a01e...|\n",
      "|       6|zxhorgqqououyqnmn...|8953e20d1c635ba25...|\n",
      "|       7|uypazquoqxxevgqff...|f2555e65c65446e71...|\n",
      "|       8|wwchozldqkostuang...|5187c7545e9f82b6b...|\n",
      "|       9|xglmmsgbiuclmwkpn...|ee1b6eec80c3197b9...|\n",
      "|      10|ejtgoapbwptkhgnhs...|c377e2166b4a5b518...|\n",
      "|      11|gyzbmrvmgkthjobod...|c6592baef11c6b997...|\n",
      "|      12|gladpurwsbqvesdgz...|25fc1145163435832...|\n",
      "|      13|szuihdpyqpqaubdwg...|eb00949f0279c0825...|\n",
      "|      14|ztaiiiepyksrfpgco...|5335a077d42a688d9...|\n",
      "|      15|dhoezgeyefkmwwbch...|389e5b181fa80691b...|\n",
      "|      16|shdqilxigwbokuhnf...|b69adf59ef59b6d8d...|\n",
      "|      17|dioemfaozjhrlxLln...|c0909cd254ae8a895...|\n",
      "|      18|bmsmtucsgrdujkslt...|994f1912713033746...|\n",
      "|      19|arlvnwswjqfBoqerh...|7007ee518cdc6f1fc...|\n",
      "|      20|xcmvetvlrwsyccpgr...|54c2e2da5f0e0d852...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Alternativamente probando que funciona el filtro por order_id par e impar y los hash md5 y sha256\n",
    "\n",
    "dff = df_6.withColumn(\"hashed_bill\", when(df_6.order_id % 2 == 0, md5(df_6.bill_raw_text)) \\\n",
    "                                      .otherwise(sha2(df_6.bill_raw_text,256)))\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
